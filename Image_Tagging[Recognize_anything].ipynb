{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ljt6qAXYFbCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02594ca3-70c7-493c-f783-61c317548d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'recognize-anything'...\n",
            "remote: Enumerating objects: 737, done.\u001b[K\n",
            "remote: Counting objects: 100% (447/447), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 737 (delta 310), reused 290 (delta 224), pack-reused 290 (from 1)\u001b[K\n",
            "Receiving objects: 100% (737/737), 27.14 MiB | 23.63 MiB/s, done.\n",
            "Resolving deltas: 100% (397/397), done.\n",
            "/content/recognize-anything\n",
            "Note: switching to 'ec6b4241c5036e337a4543838deb9bff4990de97'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at ec6b424 update\n",
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting fairscale\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.6)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332105 sha256=ddd77156a3fb309751403272c6aee9837a78510e6f485fca740780e8b028d4f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale, timm, pycocoevalcap\n",
            "Successfully installed fairscale-0.4.13 pycocoevalcap-1.2 timm-1.0.9\n"
          ]
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display, Image\n",
        "import os\n",
        "!git clone https://github.com/xinyu1205/recognize-anything.git\n",
        "%cd recognize-anything\n",
        "!git checkout ec6b4241c5036e337a4543838deb9bff4990de97\n",
        "!pip install timm transformers fairscale pycocoevalcap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MGtej2SL7DSC"
      },
      "outputs": [],
      "source": [
        "model = \"Tag2Text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plSIPdmK7-Pn",
        "outputId": "a908b243-2fb5-4fed-e6e8-e979496e7476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You selected Tag2Text\n",
            "--2024-09-11 11:44:03--  https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/tag2text_swin_14m.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.17, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /spaces/xinyu1205/recognize-anything/resolve/main/tag2text_swin_14m.pth [following]\n",
            "--2024-09-11 11:44:03--  https://huggingface.co/spaces/xinyu1205/recognize-anything/resolve/main/tag2text_swin_14m.pth\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/e6/78/e678f8565485a3f321b1180e4c7e1e18a89a9295028358eedffb98981b37e11a/4ce96f0ce98f940a6680d567f66a38ccc9ca8c4e638e5f5c5c2e881a0e3502ac?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27tag2text_swin_14m.pth%3B+filename%3D%22tag2text_swin_14m.pth%22%3B&Expires=1726314243&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNjMxNDI0M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9lNi83OC9lNjc4Zjg1NjU0ODVhM2YzMjFiMTE4MGU0YzdlMWUxOGE4OWE5Mjk1MDI4MzU4ZWVkZmZiOTg5ODFiMzdlMTFhLzRjZTk2ZjBjZTk4Zjk0MGE2NjgwZDU2N2Y2NmEzOGNjYzljYThjNGU2MzhlNWY1YzVjMmU4ODFhMGUzNTAyYWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Iwz2NWPJxisIzdhERckw5WifqmhNAU8l%7EgeUtkroOqP%7EK9VYCTcq2DebCZhiQkCcCutEUJNZPH%7E7dA9G949S9by7GGI30S0r1Dv%7E9ydV1xqKZ55qoCjd%7EI5s7b12L4T7zlnpjiZRAH0Sp184Su4go6q%7EGjI1eFl8ZXHZMhJosPU27j2W3J%7EDUoz3GhlV7GzEUR07S4ni-bwNfybY5Al5s6aWj2vMn5KTB73sL8CwZ%7EZi58ztl-%7EXc7tX6%7EADv8EVKq9xdSkaJrIUXYvBAT3N7jkf67UOneo29el7ykEVnQP1tB1r5uKqGzvznmDhnpA6vFVZCI79yL8LbLit6dV-AQ__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-09-11 11:44:03--  https://cdn-lfs.huggingface.co/repos/e6/78/e678f8565485a3f321b1180e4c7e1e18a89a9295028358eedffb98981b37e11a/4ce96f0ce98f940a6680d567f66a38ccc9ca8c4e638e5f5c5c2e881a0e3502ac?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27tag2text_swin_14m.pth%3B+filename%3D%22tag2text_swin_14m.pth%22%3B&Expires=1726314243&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNjMxNDI0M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9lNi83OC9lNjc4Zjg1NjU0ODVhM2YzMjFiMTE4MGU0YzdlMWUxOGE4OWE5Mjk1MDI4MzU4ZWVkZmZiOTg5ODFiMzdlMTFhLzRjZTk2ZjBjZTk4Zjk0MGE2NjgwZDU2N2Y2NmEzOGNjYzljYThjNGU2MzhlNWY1YzVjMmU4ODFhMGUzNTAyYWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Iwz2NWPJxisIzdhERckw5WifqmhNAU8l%7EgeUtkroOqP%7EK9VYCTcq2DebCZhiQkCcCutEUJNZPH%7E7dA9G949S9by7GGI30S0r1Dv%7E9ydV1xqKZ55qoCjd%7EI5s7b12L4T7zlnpjiZRAH0Sp184Su4go6q%7EGjI1eFl8ZXHZMhJosPU27j2W3J%7EDUoz3GhlV7GzEUR07S4ni-bwNfybY5Al5s6aWj2vMn5KTB73sL8CwZ%7EZi58ztl-%7EXc7tX6%7EADv8EVKq9xdSkaJrIUXYvBAT3N7jkf67UOneo29el7ykEVnQP1tB1r5uKqGzvznmDhnpA6vFVZCI79yL8LbLit6dV-AQ__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.167.212.45, 3.167.212.56, 3.167.212.53, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.167.212.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4478705095 (4.2G) [binary/octet-stream]\n",
            "Saving to: ‘pretrained/tag2text_swin_14m.pth’\n",
            "\n",
            "pretrained/tag2text 100%[===================>]   4.17G   169MB/s    in 30s     \n",
            "\n",
            "2024-09-11 11:44:33 (143 MB/s) - ‘pretrained/tag2text_swin_14m.pth’ saved [4478705095/4478705095]\n",
            "\n",
            "Tag2Text weights are downloaded!\n"
          ]
        }
      ],
      "source": [
        "def download_checkpoints(model):\n",
        "    print('You selected', model)\n",
        "    if not os.path.exists('pretrained'):\n",
        "        os.makedirs('pretrained')\n",
        "\n",
        "    if model == \"RAM\":\n",
        "        ram_weights_path = 'pretrained/ram_swin_large_14m.pth'\n",
        "        if not os.path.exists(ram_weights_path):\n",
        "            !wget https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/ram_swin_large_14m.pth -O pretrained/ram_swin_large_14m.pth\n",
        "        else:\n",
        "            print(\"RAM weights already downloaded!\")\n",
        "    else:\n",
        "        tag2text_weights_path = 'pretrained/tag2text_swin_14m.pth'\n",
        "        if not os.path.exists(tag2text_weights_path):\n",
        "            !wget https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/tag2text_swin_14m.pth -O pretrained/tag2text_swin_14m.pth\n",
        "        else:\n",
        "            print(\"Tag2Text weights already downloaded!\")\n",
        "\n",
        "download_checkpoints(model)\n",
        "print(model, 'weights are downloaded!')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c46A4Z3H55PB",
        "outputId": "5288e5c4-c626-48ee-b5fa-40c8e3bb9c3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f5YNmQTq8vtH"
      },
      "outputs": [],
      "source": [
        "images_dir = \"/content/drive/MyDrive/DAM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315,
          "referenced_widgets": [
            "f38e120f8b044cf88bdf8669130f5776",
            "ade0317a84be4dacab2423ed614e90fe",
            "3f4089dd855c41e685af3c5cf3b5e17e",
            "b378287a2b484495a966aa58ae7f2573",
            "de928fe7d7ee4986b89de4ea78850684"
          ]
        },
        "id": "7zTda6aDyPa5",
        "outputId": "2bc96356-b524-407d-9235-e9834c697b69"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select Image:', options=('/content/drive/MyDrive/DAM/download (1).jpg', '/content/drive/…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f38e120f8b044cf88bdf8669130f5776"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b378287a2b484495a966aa58ae7f2573"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_files = [f\"{images_dir}/{file}\" for file in sorted(os.listdir(images_dir)) if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "image_path = image_files[0]\n",
        "\n",
        "# Create dropdown widget\n",
        "image_dropdown = widgets.Dropdown(\n",
        "    options=image_files,\n",
        "    description='Select Image:',\n",
        ")\n",
        "\n",
        "# Create image preview widget\n",
        "image_preview = widgets.Output()\n",
        "\n",
        "# Define function to update image preview\n",
        "def update_preview(change):\n",
        "    global image_path\n",
        "    image_path = change.new\n",
        "    with image_preview:\n",
        "        image_preview.clear_output()\n",
        "        display(Image(filename=image_path, width=400))\n",
        "\n",
        "# Set the initial image preview\n",
        "with image_preview:\n",
        "    display(Image(filename=image_files[0], width=400))\n",
        "\n",
        "# Attach the update function to the dropdown\n",
        "image_dropdown.observe(update_preview, names='value')\n",
        "\n",
        "# Display the widgets\n",
        "display(image_dropdown, image_preview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyt3TJpWsxs8",
        "outputId": "fc621149-e2d3-45f2-8e8b-6ba6b2dd52ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 270kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 3.10MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 43.9MB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 3.47MB/s]\n",
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "--------------\n",
            "pretrained/tag2text_swin_14m.pth\n",
            "--------------\n",
            "load checkpoint from pretrained/tag2text_swin_14m.pth\n",
            "vit: swin_b\n",
            "Model Identified Tags:  shirt | beard | portrait | man | smile | wear | stand | red | young\n",
            "User Specified Tags:  None\n",
            "Image Caption:  a young man with a beard wearing a red and white checkered shirt\n"
          ]
        }
      ],
      "source": [
        "task = \"one image\" #or can be multiple image\n",
        "def run_inference(model, task):\n",
        "    if model == \"Tag2Text\" and task == \"one image\":\n",
        "        !python inference_tag2text.py  --image \"{image_path}\" \\\n",
        "        --pretrained pretrained/tag2text_swin_14m.pth\n",
        "    elif model == \"Tag2Text\" and task == \"multiple images\":\n",
        "        !python batch_inference.py --image-dir {images_dir} \\\n",
        "        --pretrained pretrained/tag2text_swin_14m.pth --model-type tag2text\n",
        "    elif model == \"RAM\" and task == \"one image\":\n",
        "        !python inference_ram.py  --image {image_path} \\\n",
        "        --pretrained pretrained/ram_swin_large_14m.pth\n",
        "    elif model == \"RAM\" and task == \"multiple images\":\n",
        "        !python batch_inference.py --image-dir {images_dir} \\\n",
        "        --pretrained pretrained/ram_swin_large_14m.pth --model-type ram\n",
        "    else:\n",
        "        print('Invalid model or task')\n",
        "\n",
        "run_inference(model, task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lxwoyIm9s7OX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afe7f55-a293-4d08-ee9f-343dbc5b12fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/encoder/layer/0/crossattention/self/query is tied\n",
            "/encoder/layer/0/crossattention/self/key is tied\n",
            "/encoder/layer/0/crossattention/self/value is tied\n",
            "/encoder/layer/0/crossattention/output/dense is tied\n",
            "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/0/intermediate/dense is tied\n",
            "/encoder/layer/0/output/dense is tied\n",
            "/encoder/layer/0/output/LayerNorm is tied\n",
            "/encoder/layer/1/crossattention/self/query is tied\n",
            "/encoder/layer/1/crossattention/self/key is tied\n",
            "/encoder/layer/1/crossattention/self/value is tied\n",
            "/encoder/layer/1/crossattention/output/dense is tied\n",
            "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
            "/encoder/layer/1/intermediate/dense is tied\n",
            "/encoder/layer/1/output/dense is tied\n",
            "/encoder/layer/1/output/LayerNorm is tied\n",
            "--------------\n",
            "pretrained/tag2text_swin_14m.pth\n",
            "--------------\n",
            "load checkpoint from pretrained/tag2text_swin_14m.pth\n",
            "vit: swin_b\n",
            "{'filepath': '/content/drive/MyDrive/DAM/download.jpg', 'model_identified_tags': 'dice | game | white | black', 'user_specified_tags': None, 'image_caption': 'a pile of black and white dice on a white background'}\n",
            "{'filepath': '/content/drive/MyDrive/DAM/download (1).jpg', 'model_identified_tags': 'cat | camera | picture | kitten | sit | look | orange', 'user_specified_tags': None, 'image_caption': 'a picture of an orange tabby cat looking at the camera'}\n",
            "{'filepath': '/content/drive/MyDrive/DAM/download (2).jpg', 'model_identified_tags': 'shirt | beard | portrait | man | smile | wear | stand | red | young', 'user_specified_tags': None, 'image_caption': 'a young man with a beard wearing a red and white checkered shirt'}\n",
            "{'filepath': '/content/drive/MyDrive/DAM/download (3).jpg', 'model_identified_tags': 'building | city | top | skyscraper | skyline | tall | large', 'user_specified_tags': None, 'image_caption': 'a large city with lots of tall buildings'}\n",
            "{'filepath': '/content/drive/MyDrive/DAM/download (4).jpg', 'model_identified_tags': 'handle | hammer | head | wooden', 'user_specified_tags': None, 'image_caption': 'a hammer with a wooden head on a white background'}\n",
            "{'filepath': '/content/drive/MyDrive/DAM/download (5).jpg', 'model_identified_tags': 'table | puppy | camera | dog | person | hold | small | white | little', 'user_specified_tags': None, 'image_caption': 'a person is holding a small white puppy over a table'}\n",
            "Processed 6 images in 33.13 seconds.\n",
            "{\n",
            "  \"status\": 0,\n",
            "  \"message\": \"ok\",\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"filepath\": \"/content/drive/MyDrive/DAM/download.jpg\",\n",
            "      \"model_identified_tags\": \"dice | game | white | black\",\n",
            "      \"user_specified_tags\": null,\n",
            "      \"image_caption\": \"a pile of black and white dice on a white background\"\n",
            "    },\n",
            "    {\n",
            "      \"filepath\": \"/content/drive/MyDrive/DAM/download (1).jpg\",\n",
            "      \"model_identified_tags\": \"cat | camera | picture | kitten | sit | look | orange\",\n",
            "      \"user_specified_tags\": null,\n",
            "      \"image_caption\": \"a picture of an orange tabby cat looking at the camera\"\n",
            "    },\n",
            "    {\n",
            "      \"filepath\": \"/content/drive/MyDrive/DAM/download (2).jpg\",\n",
            "      \"model_identified_tags\": \"shirt | beard | portrait | man | smile | wear | stand | red | young\",\n",
            "      \"user_specified_tags\": null,\n",
            "      \"image_caption\": \"a young man with a beard wearing a red and white checkered shirt\"\n",
            "    },\n",
            "    {\n",
            "      \"filepath\": \"/content/drive/MyDrive/DAM/download (3).jpg\",\n",
            "      \"model_identified_tags\": \"building | city | top | skyscraper | skyline | tall | large\",\n",
            "      \"user_specified_tags\": null,\n",
            "      \"image_caption\": \"a large city with lots of tall buildings\"\n",
            "    },\n",
            "    {\n",
            "      \"filepath\": \"/content/drive/MyDrive/DAM/download (4).jpg\",\n",
            "      \"model_identified_tags\": \"handle | hammer | head | wooden\",\n",
            "      \"user_specified_tags\": null,\n",
            "      \"image_caption\": \"a hammer with a wooden head on a white background\"\n",
            "    },\n",
            "    {\n",
            "      \"filepath\": \"/content/drive/MyDrive/DAM/download (5).jpg\",\n",
            "      \"model_identified_tags\": \"table | puppy | camera | dog | person | hold | small | white | little\",\n",
            "      \"user_specified_tags\": null,\n",
            "      \"image_caption\": \"a person is holding a small white puppy over a table\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "task = \"multiple images\"  # Set to \"multiple images\" for batch processing\n",
        "def run_inference(model, task):\n",
        "    if model == \"Tag2Text\" and task == \"one image\":\n",
        "        # Inference on a single image\n",
        "        !python inference_tag2text.py  --image \"{image_path}\" \\\n",
        "        --pretrained pretrained/tag2text_swin_14m.pth\n",
        "    elif model == \"Tag2Text\" and task == \"multiple images\":\n",
        "        # Batch inference for multiple images\n",
        "        !python batch_inference.py --image-dir \"{images_dir}\" \\\n",
        "        --pretrained pretrained/tag2text_swin_14m.pth --model-type tag2text\n",
        "    elif model == \"RAM\" and task == \"one image\":\n",
        "        # Inference on a single image\n",
        "        !python inference_ram.py  --image \"{image_path}\" \\\n",
        "        --pretrained pretrained/ram_swin_large_14m.pth\n",
        "    elif model == \"RAM\" and task == \"multiple images\":\n",
        "        # Batch inference for multiple images\n",
        "        !python batch_inference.py --image-dir \"{images_dir}\" \\\n",
        "        --pretrained pretrained/ram_swin_large_14m.pth --model-type ram\n",
        "    else:\n",
        "        print('Invalid model or task')\n",
        "\n",
        "# Run batch inference\n",
        "run_inference(model, task)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y11a1b6fAECi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f38e120f8b044cf88bdf8669130f5776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "/content/drive/MyDrive/DAM/download (1).jpg",
              "/content/drive/MyDrive/DAM/download (2).jpg",
              "/content/drive/MyDrive/DAM/download (3).jpg",
              "/content/drive/MyDrive/DAM/download (4).jpg",
              "/content/drive/MyDrive/DAM/download (5).jpg",
              "/content/drive/MyDrive/DAM/download.jpg"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select Image:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_ade0317a84be4dacab2423ed614e90fe",
            "style": "IPY_MODEL_3f4089dd855c41e685af3c5cf3b5e17e"
          }
        },
        "ade0317a84be4dacab2423ed614e90fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f4089dd855c41e685af3c5cf3b5e17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b378287a2b484495a966aa58ae7f2573": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_de928fe7d7ee4986b89de4ea78850684",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "image/jpeg": "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\n",
                  "text/plain": "<IPython.core.display.Image object>"
                },
                "metadata": {
                  "image/jpeg": {
                    "width": 400
                  }
                }
              }
            ]
          }
        },
        "de928fe7d7ee4986b89de4ea78850684": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}